{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5. analysis\n",
    "\n",
    "This notebook contains the network and node sampling procedures used to generate inputs to the integration methods evaluated in Fig. 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network sampling is straightforward: given a set of yeast co-expression networks, we randomly sampled sets of these networks for integration. All methods are provided with the same set of sampled networks to ensure differences in integration performance between methods is not influenced by the sampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kaplan T 2008',\n",
       " 'Mendes-Ferreira A 2007, 2010',\n",
       " 'Carter GW 2007',\n",
       " 'Guan Q 2006',\n",
       " 'Aragon AD 2006 (rep 1)',\n",
       " 'Knijnenburg TA 2009',\n",
       " 'Hu Z 2007']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_path = Path(\"../data/methods\")\n",
    "\n",
    "# import coex network names\n",
    "coex_names = pd.read_csv(in_path / \"yeast-coex-network-names.txt\", sep=\"\\n\", header=None)\n",
    "coex_names = list(coex_names[0])\n",
    "\n",
    "# sampling networks for increasing sample sizes\n",
    "final_samples = {}\n",
    "for sample_size in [2, 3, 7, 15, 29]:\n",
    "\n",
    "    sampled_nets = {}\n",
    "\n",
    "    # perform sampling for 10 trials\n",
    "    for trial in range(10):\n",
    "        sampled = random.sample(coex_names, sample_size)  # samples without replacement\n",
    "        sampled_nets[trial] = sampled\n",
    "\n",
    "    final_samples[sample_size] = sampled_nets\n",
    "\n",
    "# show samples from trial 5 of sample size 7 (as an example)\n",
    "final_samples[7][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node sampling is slightly more involved: here we sample a set of nodes across the four input networks, and return the subgraph induced on those nodes. Similar to network sampling, all integration methods are provided with the same set of subsampled networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12938 nodes in union of networks\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 3581\n",
      "Number of edges: 8934\n",
      "Average degree:   4.9897 \n",
      "\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 5083\n",
      "Number of edges: 17163\n",
      "Average degree:   6.7531 \n",
      "\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 2487\n",
      "Number of edges: 8041\n",
      "Average degree:   6.4664 \n",
      "\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1976\n",
      "Number of edges: 4704\n",
      "Average degree:   4.7611 \n",
      "\n",
      "Total nodes: 6000\n"
     ]
    }
   ],
   "source": [
    "# import the human PPI networks\n",
    "names = [\"Huttlin-2015.txt\", \"Huttlin-2017.txt\", \"Hein-2015.txt\", \"Rolland-2014.txt\"]\n",
    "nets = [nx.read_weighted_edgelist(in_path / name) for name in names]\n",
    "\n",
    "# add self loops\n",
    "for net in nets:\n",
    "    net.add_edges_from([(node, node) for node in net.nodes()])\n",
    "\n",
    "# get all nodes present in the networks\n",
    "node_union = list(reduce(np.union1d, [list(net.nodes()) for net in nets]))\n",
    "print(f\"{len(node_union)} nodes in union of networks\")\n",
    "\n",
    "# subsample networks for increasing node sizes\n",
    "final_nets = {}\n",
    "for n_nodes in [2000, 4000, 6000, 8000, 10000]:\n",
    "\n",
    "    subsampled_nets = {}\n",
    "\n",
    "    # perform sampling for 10 trials\n",
    "    for trial in range(10):\n",
    "\n",
    "        sampled = []\n",
    "\n",
    "        # randomly sample nodes from `node_union`\n",
    "        node_sample = random.sample(node_union, n_nodes)  # without replacement\n",
    "\n",
    "        for net in nets:\n",
    "            common_nodes = np.intersect1d(node_sample, list(net.nodes()))\n",
    "            subsampled_net = net.subgraph(common_nodes)\n",
    "            sampled.append(subsampled_net)\n",
    "        \n",
    "        subsampled_nets[trial] = sampled\n",
    "    final_nets[n_nodes] = subsampled_nets\n",
    "\n",
    "# show samples from trial 5 of node sample size 6000 (for example)\n",
    "for net in final_nets[6000][5]:\n",
    "    print(nx.info(net), \"\\n\")\n",
    "\n",
    "# each net contains less than 6000 nodes, but the union of nodes across these nets is 6000\n",
    "print(\"Total nodes:\", len(reduce(np.union1d, [list(net.nodes()) for net in final_nets[6000][5]])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f1142d212e6e8cceef5b4a22ed61237952f6fb2a7c6aecb63226793aae517a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bionic-evals': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
